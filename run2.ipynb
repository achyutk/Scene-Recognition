{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408325df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from collections import Counter\n",
    "from random import sample\n",
    "from sklearn.metrics import classification_report\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1a287",
   "metadata": {},
   "source": [
    "## 1.Creating Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7abe49c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'training/training'\n",
    "feature_set,label_set = utils.dataset(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd9b18",
   "metadata": {},
   "source": [
    "## 2.Train - Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc150722",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [x for x in range(0, len(feature_set))]  #temporary list to store the number of images in feature set\n",
    "train_ind = random.sample(range(0, len(feature_set)), int(0.70*len(feature_set))) # Identifying indexes for training set\n",
    "val_ind = [x for x in l if x not in train_ind] # Identifying indexes for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b55da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [feature_set[i] for i in train_ind] #Creating Training set\n",
    "X_val = [feature_set[i] for i in val_ind] #Creating validation set\n",
    "y_train = [label_set[i] for i in train_ind]\n",
    "y_val = [label_set[i] for i in val_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8543a029",
   "metadata": {},
   "source": [
    "## 3. Learning Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "612bcc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making dataset for learning vocab\n",
    "feature_vocab=[]\n",
    "for i in X_train:\n",
    "    for j in i:\n",
    "        feature_vocab.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f4d9bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffelling the Dataset\n",
    "random.shuffle(feature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dddcf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Kmeans model which learns the vocabulary\n",
    "kmeans = utils.learning_words(feature_vocab[:int(len(feature_vocab)*0.25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4c7202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mapped_df_train = utils.mapping(kmeans,X_train) #Mapping dataset with vocabulary\n",
    "mapped_df_train['label']=y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694a10b6",
   "metadata": {},
   "source": [
    "## 4. Creating Classifiers- 1 linear classifiers for each repective class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db0fab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes= list(set(mapped_df_train['label'])) #getting a list of different classes= 15 scenes\n",
    "classifiers = [] #List to store model for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87267313",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Loop to store classifiers in a list\n",
    "for i in classes:\n",
    "    classify = utils.classifier(mapped_df_train,i)\n",
    "    classifiers.append(classify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617eac6",
   "metadata": {},
   "source": [
    "## 7. Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97e5f4",
   "metadata": {},
   "source": [
    "### Making Predicitions on Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a1d7392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Coast       0.84      0.67      0.75        64\n",
      "      Forest       0.97      0.82      0.89        72\n",
      "     Highway       0.70      0.84      0.76        67\n",
      "  Insidecity       0.89      0.64      0.74        66\n",
      "    Mountain       0.93      0.77      0.84        73\n",
      "      Office       0.85      0.58      0.69        67\n",
      " OpenCountry       0.69      0.78      0.74        64\n",
      "      Street       0.65      0.94      0.77        68\n",
      "      Suburb       0.99      0.95      0.97        76\n",
      "TallBuilding       0.90      0.53      0.67        68\n",
      "     bedroom       0.52      0.56      0.54        78\n",
      "  industrial       0.47      0.69      0.56        68\n",
      "     kitchen       0.53      0.65      0.59        74\n",
      "  livingroom       0.47      0.49      0.48        72\n",
      "       store       0.73      0.71      0.72        73\n",
      "\n",
      "    accuracy                           0.71      1050\n",
      "   macro avg       0.74      0.71      0.71      1050\n",
      "weighted avg       0.74      0.71      0.71      1050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"# Making Predicitions Using Training set\"\"\"\n",
    "y_train_pred = utils.predict(mapped_df_train,classifiers)\n",
    "print(classification_report(y_train,y_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3941ad9",
   "metadata": {},
   "source": [
    "### Making Predictions Using Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00ce5b58",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Coast       0.68      0.42      0.52        36\n",
      "      Forest       0.85      0.61      0.71        28\n",
      "     Highway       0.66      0.64      0.65        33\n",
      "  Insidecity       0.64      0.41      0.50        34\n",
      "    Mountain       0.75      0.67      0.71        27\n",
      "      Office       0.67      0.42      0.52        33\n",
      " OpenCountry       0.60      0.75      0.67        36\n",
      "      Street       0.48      0.78      0.60        32\n",
      "      Suburb       0.74      0.71      0.72        24\n",
      "TallBuilding       0.30      0.09      0.14        32\n",
      "     bedroom       0.36      0.59      0.45        22\n",
      "  industrial       0.11      0.16      0.13        32\n",
      "     kitchen       0.45      0.58      0.51        26\n",
      "  livingroom       0.30      0.32      0.31        28\n",
      "       store       0.38      0.48      0.43        27\n",
      "\n",
      "    accuracy                           0.50       450\n",
      "   macro avg       0.53      0.51      0.50       450\n",
      "weighted avg       0.53      0.50      0.50       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mapped_df_val = utils.mapping(kmeans,X_val) #Creating histogram using vocabulary for validation set\n",
    "mapped_df_val['label']=y_val\n",
    "y_val_pred = utils.predict(mapped_df_val,classifiers)\n",
    "print(classification_report(y_val,y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de540ef",
   "metadata": {},
   "source": [
    "### Making predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333c224",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_path = 'testing/testing'\n",
    "feature_set,label_set = dataset_test(test_path) #Creating testing dataset\n",
    "\n",
    "mapped_test_set = utils.mapping(kmeans,feature_set) #Creating histogram using vocabulary for validation set\n",
    "mapped_test_set['label']=0\n",
    "y_test_pred = utils.predict(mapped_test_set,classifiers) #Making Prediciton on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb351f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing predictions in a datframe for testing dataset\n",
    "lists =[m+\" \"+n for m,n in zip(label_set,y_test_pred)]\n",
    "re=pd.DataFrame(lists,columns=['Name'])\n",
    "re['Num']= [int(x.split('.')[0]) for x in re['Name'] ]\n",
    "re = re.sort_values(by=['Num'])\n",
    "re = re.reset_index()\n",
    "sub = list(re['Name'])\n",
    "\n",
    "#Saving the predictions in a txtfile\n",
    "with open(r'results/run2.txt', 'w') as fp:\n",
    "    for item in sub:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
